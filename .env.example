# Common
NODE_ENV=development
LOG_LEVEL=info

# API
API_PORT=3001
HOST=127.0.0.1

# Redis / Queue
REDIS_URL=redis://localhost:6379

# Database
# SQLite for local-first MVP; upgrade to Postgres later
DATABASE_URL=file:./data/dev.db

# Ollama Local LLM
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Vector DB
CHROMA_DB_PATH=./data/chroma

# Monitoring
PROMETHEUS_BASE_URL=http://localhost:9090

# Backend (FastAPI) safety defaults
DRY_RUN=false
ENABLE_LOCAL_EXECUTION=false
CORS_ORIGINS=http://localhost:3000

# Frontend (Next.js) overrides
NEXT_PUBLIC_API_BASE_URL=http://127.0.0.1:3001
NEXT_PUBLIC_POLL_INTERVAL_MS=2000
NEXT_PUBLIC_DRY_RUN=true

# --- LLM Provider Selection ---
# Choose one provider: OLLAMA | OPENAI | GEMINI
LLM_PROVIDER=OPENAI

# If LLM_PROVIDER=OPENAI
# For OpenAI hosted: use https://api.openai.com/v1
# For local OpenAI-compatible servers: set OPENAI_BASE_URL accordingly
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# If LLM_PROVIDER=GEMINI
GEMINI_API_KEY=
GEMINI_MODEL=gemini-1.5-pro
